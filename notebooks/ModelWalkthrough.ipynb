{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Au3pOy1363J"
   },
   "source": [
    "# Keras Model Implementation Walkthrough\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukewood/ModelWalkthrough/blob/master/notebooks/ModelWalkthrough.ipynb)\n",
    "\n",
    "[This guide has a companion blog post.](https://lukewood.xyz/blog/keras-model-walkthrough)\n",
    "\n",
    "The Keras Model class is one of the centerpieces of the framework.\n",
    "It encapsulates metric tracking, callbacks, distribution, training loops, various input types, and a wide variety of other training related behavior. \n",
    "This has led to the Model class containing a large volume of code that can be intimidating to sift through.\n",
    "\n",
    "This guide walks through a simplified model implementation in order to help you understand what model does under the hood.\n",
    "After following along with this guide you will understand how the keras model class achieves the behavior listed above.\n",
    "\n",
    "The SimplifiedModel can also serve as a starting point for those looking to implement custom models or training loops.\n",
    "A forkable template using the SimplifiedModel class be found on my github at [https://github.com/lukewood/ModelWalkthrough](https://github.com/lukewood/ModelWalkthrough).\n",
    "\n",
    "For the sake of brevity, the class written in this guide subclasses `keras.layers.Layer` to leverage some helper functions, such as Keras' `__call__` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkdQ5wp74ygi"
   },
   "source": [
    "## Core Implementation\n",
    "Let's create a basic implementation that supports `compile()`, `fit()`, `predict()`, and `eval()` before we introduce distribution strategy, metric tracking, callbacks, and other features.\n",
    "\n",
    "Note that the `SimplifiedModel` class operates as a `keras.Model` subclass, overriding the `call()` method to produce predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m3u7zngg35br"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "class SimplifiedModel(keras.layers.Layer):\n",
    "  \"\"\"SimplifiedModel is a stripped down barebones version of keras.Model.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(SimplifiedModel, self).__init__(*args, **kwargs)\n",
    "    self.dense = keras.layers.Dense(1)\n",
    "    self.distribute_strategy = tf.distribute.get_strategy()\n",
    "  \n",
    "  def compile(self,\n",
    "              optimizer='rmsprop',\n",
    "              loss=None,\n",
    "              metrics=None,\n",
    "              loss_weights=None):\n",
    "    with self.distribute_strategy.scope():\n",
    "      self.optimizer = optimizers.get(optimizer)\n",
    "      self.loss = loss\n",
    "      self.metrics_list = metrics if isinstance(metrics, list) else [metrics]\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return self.dense(inputs)\n",
    "\n",
    "  def predict_step(self, x):\n",
    "    return self(x, training=False)\n",
    "\n",
    "  def train_step(self, data):\n",
    "    x, y = data\n",
    "    # Run forward pass.\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = self(x, training=True)\n",
    "      loss = self.loss(y, y_pred)\n",
    "      for extra_loss in self.losses:\n",
    "        loss += scale_loss_for_distribution(extra_loss)\n",
    "\n",
    "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
    "\n",
    "    return_metrics = {'loss': loss}\n",
    "    for metric in self.metrics_list:\n",
    "      metric.update_state(y, y_pred, None)\n",
    "      result = metric.result()\n",
    "      if isinstance(result, dict):\n",
    "        return_metrics.update(result)\n",
    "      else:\n",
    "        return_metrics[metric.name] = result\n",
    "    return return_metrics\n",
    "\n",
    "  def fit(self, dataset: tf.data.Dataset, epochs=1, verbose=1):\n",
    "    \"\"\"This simplified version of fit only accepts a TensorFlow dataset.\n",
    "\n",
    "    Args:\n",
    "      dataset: tf.data.Dataset, must yield a tuple of inputs and a one hot\n",
    "        encoded vector containing labels\n",
    "      epochs: number of passes to perform over the verbosity\n",
    "      verbose: verbosity of logging during fit\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "      for batch in dataset:\n",
    "        metrics = self.train_step(batch)\n",
    "        metric_str = ', '.join(\n",
    "            [f'{metric_name}: {val}' for metric_name, val in metrics.items()])\n",
    "        # Minimal progress logging implementation\n",
    "        print(f'\\repoch: ({epoch+1}/{epochs}), {metric_str}', end='')\n",
    "    print()\n",
    "\n",
    "  def test_step(self, x, y):\n",
    "    y_pred = self(x, training=False)\n",
    "    loss = self.loss(y, y_pred)\n",
    "    for extra_loss in self.losses:\n",
    "      loss += scale_loss_for_distribution(extra_loss)\n",
    "\n",
    "    return_metrics = {\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    for metric in self.metrics_list:\n",
    "      metric.update_state(y, y_pred, None)\n",
    "      result = metric.result()\n",
    "      if isinstance(result, dict):\n",
    "        return_metrics.update(result)\n",
    "      else:\n",
    "        return_metrics[metric.name] = result\n",
    "    return return_metrics\n",
    "\n",
    "  def evaluate(self, dataset):\n",
    "    self.reset_metrics()\n",
    "    metrics_aggregate = []\n",
    "    for xs, ys in dataset:\n",
    "      self.reset_metrics()\n",
    "      metrics_aggregate.append(self.test_step(xs, ys))\n",
    "\n",
    "    if not metrics_aggregate:\n",
    "      raise ValueError('dataset must contain at least one batch of samples.  '\n",
    "                       f'Received: {dataset}')\n",
    "\n",
    "    result = {}\n",
    "    for k in metrics_aggregate[0]:\n",
    "      result[k] = 0.\n",
    "\n",
    "    for metric_iter in metrics_aggregate:\n",
    "      for k, v in metric_iter.items():\n",
    "        result[k] += v / len(metrics_aggregate)\n",
    "    return result\n",
    "\n",
    "  def predict(self, dataset):\n",
    "    result = []\n",
    "    for xs in dataset:\n",
    "      result.append(self(xs, training=False))\n",
    "    return tf.concat(result, axis=0)\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    for metric in self.metrics_list:\n",
    "      metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwq0dJpw6ulK"
   },
   "source": [
    "When you first construct a model, it exists in an uncompiled state.\n",
    "In this state the optimizer, compiled metrics, and loss have not yet been created.\n",
    "The `compile()` method creates the optimizer, takes a loss function, and creates a reference to a list of metrics.  These are all later used in training.\n",
    "\n",
    "`train_step`, `predict_step`, and `eval_step` all contain the logic to perform a single step of their corresponding methods: `fit()`, `predict()`, and `evaluate()` respectively.\n",
    "Note that while `predict_step()` simply invokes call, `train_step()` and `eval_step()` track loss and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UtP-lue369i"
   },
   "source": [
    "The simplified model class expects a `tf.data.Dataset` as an input to `fit()`, `predict()`, and `evaluate()`.\n",
    "The model offloads the batching behavior to the dataset.\n",
    "To use the model, you'd do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skAQEIoC6SbJ",
    "outputId": "9db05284-156f-4340-8ba1-21fb8e5bf964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Before Fit: {'loss': <tf.Tensor: shape=(), dtype=float32, numpy=10.000002>, 'mean_absolute_percentage_error': <tf.Tensor: shape=(), dtype=float32, numpy=100.0>}\n",
      "epoch: (10/10), loss: 1.4210854715202004e-13, mean_absolute_percentage_error: 0.5994006395339966\n",
      "Metrics After Fit: {'loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4210857e-13>, 'mean_absolute_percentage_error': <tf.Tensor: shape=(), dtype=float32, numpy=1.1920929e-05>}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.losses as losses\n",
    "\n",
    "model = SimplifiedModel()\n",
    "\n",
    "x, y = np.zeros((1000, 10)), np.ones((1000, 1))\n",
    "ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "# Our model expects the dataset to be batched\n",
    "ds = ds.batch(10)\n",
    "\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x))\n",
    "ds_test = ds_test.batch(10)\n",
    "\n",
    "model.compile('sgd', \n",
    "              loss=losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM), \n",
    "              metrics=[tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    ")\n",
    "model.build(input_shape=(None, 10))\n",
    "metrics = model.evaluate(ds)\n",
    "print('Metrics Before Fit:', metrics)\n",
    "\n",
    "model.fit(ds, epochs=10, verbose=2)\n",
    "\n",
    "metrics = model.evaluate(ds)\n",
    "print('Metrics After Fit:', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnhstG5F6obs"
   },
   "source": [
    "This model class implements our expected behavior, but it's missing some critical logic that `keras.Model` implements.\n",
    "\n",
    "Perhaps most notably, this model does not support distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx2C890V8M9h"
   },
   "source": [
    "## Batched Execution, Compiled `train_step()`\n",
    "\n",
    "Currently we are executing our `train_step()` calls one at a time, the `train_step()` function is not a compiled `tf.function`, and the model does not work the TensorFlow distribution strategies.  In this section we will implement all of these performance enhancements and net massive performance gains.\n",
    "\n",
    "First, we begin by wrapping `train_step()` in a compiled function:\n",
    "\n",
    "```python\n",
    "class SimplifiedModel(keras.Model):\n",
    "  # ... \n",
    "  def make_train_function(self):\n",
    "    if self.train_function:\n",
    "      return self.train_function\n",
    "\n",
    "    def step_function(model, iterator):\n",
    "\n",
    "      def run_step(data):\n",
    "        outputs = model.train_step(data)\n",
    "        model._train_counter.assign_add(1)  # pylint: disable=protected-access\n",
    "        return outputs\n",
    "\n",
    "      data = next(iterator)\n",
    "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
    "      return model.distribute_strategy.unwrap(outputs)[0]\n",
    "\n",
    "    def train_function(iterator):\n",
    "      \"\"\"Runs a training execution with multiple steps.\"\"\"\n",
    "      # Autograph cannot infer the return type of undeclared non-Tensor\n",
    "      # variables from inside loops. The limitations documentation explains this\n",
    "      # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md\n",
    "      names = [m.name for m in self.metrics_list] + ['loss']\n",
    "      outputs = dict.fromkeys(names, 0.)\n",
    "      for _ in tf.range(self.steps_per_execution):\n",
    "        outputs = step_function(self, iterator)\n",
    "      return outputs\n",
    "\n",
    "    train_function = tf.function(train_function, experimental_relax_shapes=True)\n",
    "\n",
    "    # A separate function is needed to prevent self-referential\n",
    "    # infinitely-recursive closures\n",
    "    cluster_train_function = None\n",
    "    if self._cluster_coordinator:\n",
    "      # pylint: disable=g-long-lambda\n",
    "      cluster_train_function = lambda it: self._cluster_coordinator.schedule(\n",
    "          train_function, args=(it,))\n",
    "\n",
    "    self.train_function = cluster_train_function or train_function\n",
    "    return self.train_function\n",
    "```\n",
    "\n",
    "This function also runs the `train_step()` function using the model's provided distribution strategy.\n",
    "\n",
    "Next, we need to update our `fit()` method to utilize this new train function:\n",
    "\n",
    "```python\n",
    "  def fit(self, dataset: tf.data.Dataset, epochs=1, verbose=1):\n",
    "    \"\"\"This simplified version of fit only accepts a TensorFlow dataset.\n",
    "\n",
    "    Args:\n",
    "      dataset: tf.data.Dataset, must yield a tuple of inputs and a one hot\n",
    "        encoded vector containing labels\n",
    "      epochs: number of passes to perform over the verbosity\n",
    "      verbose: verbosity of logging during fit\n",
    "    \"\"\"\n",
    "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n",
    "      self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n",
    "          self.distribute_strategy)\n",
    "      dataset = self._cluster_coordinator.create_per_worker_dataset(dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        iterator = iter(dataset)\n",
    "        for step in range(0, steps_per_epoch, self.steps_per_execution):\n",
    "          try:\n",
    "            # returns {'loss': loss, 'metric1': val1, ...}\n",
    "            metrics = self.train_function(iterator)\n",
    "            metric_str = ', '.join(\n",
    "              [f'{metric_name}: {val}' for metric_name, val in metrics.items()])\n",
    "            print(f'\\repoch: ({epoch+1}/{epochs}), {metric_str}', end='')\n",
    "          except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        print()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWlkTE-kobpJ"
   },
   "source": [
    "Next, we will implement computation batching.  By batching computation, we reduce the number of context transfers between the computation host and the python side callbacks.  In the Keras model class this is done by the `steps_per_execution` parameter passed to the `compile()` method.\n",
    "\n",
    "First, we need to update `compile()` to include this new parameter:\n",
    "\n",
    "```python\n",
    "  def compile(self,\n",
    "              optimizer='rmsprop',\n",
    "              loss=None,\n",
    "              metrics=None,\n",
    "              loss_weights=None,\n",
    "              weighted_metrics=None,\n",
    "              steps_per_execution=1):\n",
    "    # We need to compile the loss and metrics within the strategy scope\n",
    "    with self.distribute_strategy.scope():\n",
    "      self.steps_per_execution = steps_per_execution\n",
    "      ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKkHBGMku8lD"
   },
   "source": [
    "## Keras Callbacks\n",
    "Keras callbacks are objects that perform actions at various stages of training.  There is a large library of existing callbacks to handle things like:\n",
    "- Write TensorBoard logs after every batch of training to monitor your metrics\n",
    "- Periodically save your model to disk\n",
    "- Do early stopping\n",
    "- Get a view on internal states and statistics of a model during training\n",
    "- ...and more\n",
    "\n",
    "You can read more about callbacks here: https://keras.io/api/callbacks/\n",
    "\n",
    "Let's integrate Keras callbacks into our SimplifiedModel class.\n",
    "\n",
    "In order to do so, we will need to add a callbacks parameter to our `fit()` method.\n",
    "Additionally, we will wrap the `callbacks` into a Keras `CallbackList`:\n",
    "```python\n",
    "\n",
    "  def fit(\n",
    "      self,\n",
    "      dataset,\n",
    "      epochs=1,\n",
    "      verbose=1,\n",
    "      steps_per_epoch=sys.maxsize,  # default to max to iterate entire dataset\n",
    "      callbacks=None):\n",
    "    \"\"\"This simplified version of fit only accepts a TensorFlow dataset.\n",
    "\n",
    "    Args:\n",
    "      dataset: tf.data.Dataset, must yield a tuple of inputs and a one hot\n",
    "        encoded vector containing labels\n",
    "      epochs: number of passes to perform over the verbosity\n",
    "      verbose: verbosity of logging during fit\n",
    "      steps_per_epoch: number of steps that counts as an epoch, useful with\n",
    "        endless datasets.  When using a finite dataset, leave as sys.maxsize.\n",
    "      callbacks: list of Keras callbacks\n",
    "    \"\"\"\n",
    "    callbacks = callbacks_module.CallbackList(\n",
    "        callbacks,\n",
    "        add_history=True,\n",
    "        add_progbar=verbose != 0,\n",
    "        model=self,\n",
    "        verbose=verbose,\n",
    "        epochs=epochs)\n",
    "\n",
    "    dataset = self.distribute_strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n",
    "      self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n",
    "          self.distribute_strategy)\n",
    "      dataset = self._cluster_coordinator.create_per_worker_dataset(dataset)\n",
    "\n",
    "    self.make_train_function()\n",
    "    self._train_counter.assign(0)\n",
    "    callbacks.on_train_begin()\n",
    "    for epoch in range(epochs):\n",
    "      iterator = iter(dataset)\n",
    "      callbacks.on_epoch_begin(epoch)\n",
    "      for step in range(0, steps_per_epoch, self.steps_per_execution):\n",
    "        callbacks.on_train_batch_begin(step)\n",
    "        try:\n",
    "          # returns {'loss': loss, 'metric1': val1, ...}\n",
    "          unused_metrics = self.train_function(iterator)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "          break\n",
    "        callbacks.on_train_batch_end(step)\n",
    "      callbacks.on_epoch_end(epoch, None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XUhDXJj2nd1"
   },
   "source": [
    "We can now pass any Keras callbacks to `fit()` and have it behave as expected.  Additionally, we now get the Keras progress bar when running fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lY8WoXRP30zX"
   },
   "source": [
    "## Final Usage, Recap\n",
    "The final code for the SimplifiedModel class is available below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DjYdPj8E6n5C"
   },
   "outputs": [],
   "source": [
    "\"\"\"SimplifiedModel is a barebones Keras model class.\n",
    "\n",
    "The intended use of this class is for end users to fork this class and replace\n",
    "`compile()`, `fit()` and `predict()` with their own logic.\n",
    "\"\"\"\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SimplifiedModel(tf.keras.layers.Layer):\n",
    "  \"\"\"SimplifiedModel is a stripped down barebones version of keras.Model.\"\"\"\n",
    "  \n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(SimplifiedModel, self).__init__(*args, **kwargs)\n",
    "    self.dense = tf.keras.layers.Dense(1)\n",
    "    self.distribute_strategy = tf.distribute.get_strategy()\n",
    "    agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "    self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg, trainable=False)\n",
    "    self._cluster_coordinator = None\n",
    "    if self.distribute_strategy._should_use_with_coordinator:\n",
    "      self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n",
    "          self.distribute_strategy)\n",
    "\n",
    "  def compile(self,\n",
    "              optimizer='rmsprop',\n",
    "              loss=None,\n",
    "              metrics=None,\n",
    "              loss_weights=None,\n",
    "              weighted_metrics=None,\n",
    "              steps_per_execution=1):\n",
    "    # We need to compile the loss and metrics within the strategy scope\n",
    "    with self.distribute_strategy.scope():\n",
    "      self.optimizer = optimizers.get(optimizer)\n",
    "      self.loss = loss\n",
    "      self.metrics_list = metrics if isinstance(metrics, list) else [metrics]\n",
    "      self.steps_per_execution = steps_per_execution\n",
    "      self.train_function = None\n",
    "      self._is_compiled = True\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return self.dense(inputs)\n",
    "\n",
    "  def predict_step(self, x):\n",
    "    return self(x, training=False)\n",
    "\n",
    "  def train_step(self, data):\n",
    "    x, y = data\n",
    "    # Run forward pass.\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = self(x, training=True)\n",
    "      loss = self.loss(y, y_pred)\n",
    "      for extra_loss in self.losses:\n",
    "        loss += scale_loss_for_distribution(extra_loss)\n",
    "\n",
    "    # Run backwards pass.\n",
    "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
    "    # Collect metrics to return\n",
    "    return_metrics = {'loss': loss}\n",
    "    for metric in self.metrics:\n",
    "      metric.update_state(y, y_pred, None)\n",
    "      result = metric.result()\n",
    "      if isinstance(result, dict):\n",
    "        return_metrics.update(result)\n",
    "      else:\n",
    "        return_metrics[metric.name] = result\n",
    "    return return_metrics\n",
    "\n",
    "  def fit(\n",
    "      self,\n",
    "      dataset,\n",
    "      epochs=1,\n",
    "      verbose=1,\n",
    "      steps_per_epoch=sys.maxsize,  # default to max to iterate entire dataset\n",
    "      callbacks=None):\n",
    "    \"\"\"This simplified version of fit only accepts a TensorFlow dataset.\n",
    "\n",
    "    Args:\n",
    "      dataset: tf.data.Dataset, must yield a tuple of inputs and a one hot\n",
    "        encoded vector containing labels\n",
    "      epochs: number of passes to perform over the verbosity\n",
    "      verbose: verbosity of logging during fit\n",
    "      steps_per_epoch: number of steps that counts as an epoch, useful with\n",
    "        endless datasets.  When using a finite dataset, leave as sys.maxsize.\n",
    "      callbacks: list of Keras callbacks\n",
    "    \"\"\"\n",
    "    callbacks = tf.keras.callbacks.CallbackList(\n",
    "        callbacks,\n",
    "        add_history=True,\n",
    "        add_progbar=verbose != 0,\n",
    "        model=self,\n",
    "        verbose=verbose,\n",
    "        epochs=epochs)\n",
    "\n",
    "    dataset = self.distribute_strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n",
    "      self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n",
    "          self.distribute_strategy)\n",
    "      dataset = self._cluster_coordinator.create_per_worker_dataset(dataset)\n",
    "\n",
    "    self.make_train_function()\n",
    "    self._train_counter.assign(0)\n",
    "    callbacks.on_train_begin()\n",
    "    for epoch in range(epochs):\n",
    "      iterator = iter(dataset)\n",
    "      callbacks.on_epoch_begin(epoch)\n",
    "      for step in range(0, steps_per_epoch, self.steps_per_execution):\n",
    "        callbacks.on_train_batch_begin(step)\n",
    "        try:\n",
    "          # returns {'loss': loss, 'metric1': val1, ...}\n",
    "          unused_metrics = self.train_function(iterator)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "          break\n",
    "        callbacks.on_train_batch_end(step)\n",
    "      callbacks.on_epoch_end(epoch, None)\n",
    "\n",
    "  def make_train_function(self):\n",
    "    if self.train_function:\n",
    "      return self.train_function\n",
    "\n",
    "    def step_function(model, iterator):\n",
    "\n",
    "      def run_step(data):\n",
    "        outputs = model.train_step(data)\n",
    "        model._train_counter.assign_add(1)  # pylint: disable=protected-access\n",
    "        return outputs\n",
    "\n",
    "      data = next(iterator)\n",
    "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
    "      return model.distribute_strategy.unwrap(outputs)[0]\n",
    "\n",
    "    def train_function(iterator):\n",
    "      \"\"\"Runs a training execution with multiple steps.\"\"\"\n",
    "      # Autograph cannot infer the return type of undeclared non-Tensor\n",
    "      # variables from inside loops. The limitations documentation explains this\n",
    "      # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md\n",
    "      names = [m.name for m in self.metrics] + ['loss']\n",
    "      outputs = dict.fromkeys(names, 0.)\n",
    "      for _ in tf.range(self.steps_per_execution):\n",
    "        outputs = step_function(self, iterator)\n",
    "      return outputs\n",
    "\n",
    "    train_function = tf.function(train_function, experimental_relax_shapes=True)\n",
    "\n",
    "    # A separate function is needed to prevent self-referential\n",
    "    # infinitely-recursive closures\n",
    "    cluster_train_function = None\n",
    "    if self._cluster_coordinator:\n",
    "      # pylint: disable=g-long-lambda\n",
    "      cluster_train_function = lambda it: self._cluster_coordinator.schedule(\n",
    "          train_function, args=(it,))\n",
    "\n",
    "    self.train_function = cluster_train_function or train_function\n",
    "    return self.train_function\n",
    "\n",
    "  def test_step(self, x, y):\n",
    "    y_pred = self(x, training=False)\n",
    "    loss = self.loss(y, y_pred)\n",
    "    for extra_loss in self.losses:\n",
    "      loss += scale_loss_for_distribution(extra_loss)\n",
    "\n",
    "    return_metrics = {\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    for metric in self.metrics:\n",
    "      metric.update_state(y, y_pred, None)\n",
    "      result = metric.result()\n",
    "      if isinstance(result, dict):\n",
    "        return_metrics.update(result)\n",
    "      else:\n",
    "        return_metrics[metric.name] = result\n",
    "    return return_metrics\n",
    "\n",
    "  def evaluate(self, dataset):\n",
    "    self.reset_metrics()\n",
    "    metrics_aggregate = []\n",
    "    for xs, ys in dataset:\n",
    "      self.reset_metrics()\n",
    "      metrics_aggregate.append(self.test_step(xs, ys))\n",
    "\n",
    "    if not metrics_aggregate:\n",
    "      raise ValueError('dataset must contain at least one batch of samples.  '\n",
    "                       f'Received: {dataset}')\n",
    "\n",
    "    result = {}\n",
    "    for k in metrics_aggregate[0]:\n",
    "      result[k] = 0.\n",
    "\n",
    "    for metric_iter in metrics_aggregate:\n",
    "      for k, v in metric_iter.items():\n",
    "        result[k] += v / len(metrics_aggregate)\n",
    "    return result\n",
    "\n",
    "  def predict(self, dataset):\n",
    "    result = []\n",
    "    for xs in dataset:\n",
    "      result.append(self(xs, training=False))\n",
    "    return tf.concat(result, axis=0)\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    for metric in self.metrics_list:\n",
    "      metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoE9NXCs42Xm"
   },
   "source": [
    "Below is an example use of the SimplifiedModel class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwDpgqmF41bI",
    "outputId": "26280e5f-c13f-4da8-b39c-59a8ce5066d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OneDeviceStrategy\n",
      "Metrics Before Fit: {'loss': <tf.Tensor: shape=(), dtype=float32, numpy=10.000002>, 'mean_absolute_percentage_error': <tf.Tensor: shape=(), dtype=float32, numpy=100.0>}\n",
      "Epoch 1/10\n",
      "101/101 - 1s - 538ms/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 214ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 234ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 223ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 221ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 223ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 220ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 213ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 248ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 251ms/epoch - 2ms/step\n",
      "Predictions Shape: (1000, 1)\n",
      "Metrics After: {'loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4210857e-13>, 'mean_absolute_percentage_error': <tf.Tensor: shape=(), dtype=float32, numpy=1.1920929e-05>}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.losses as losses\n",
    "\n",
    "try:\n",
    "  strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "  print(\"Using OneDeviceStrategy\")\n",
    "except:\n",
    "  strategy = tf.distribute.get_strategy()\n",
    "  print(\"Using\", strategy)\n",
    "\n",
    "# Make sure to create all `tf.Variable`s under the `scope`.\n",
    "with strategy.scope():\n",
    "  model = SimplifiedModel()\n",
    "\n",
    "  x, y = np.zeros((1000, 10)), np.ones((1000, 1))\n",
    "  ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "  # Our model expects the dataset to be batched\n",
    "  ds = ds.batch(10)\n",
    "\n",
    "  ds_test = tf.data.Dataset.from_tensor_slices((x))\n",
    "  ds_test = ds_test.batch(10)\n",
    "\n",
    "  model.compile('sgd',\n",
    "    loss=losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM),\n",
    "    metrics=[tf.keras.metrics.MeanAbsolutePercentageError()],\n",
    "    steps_per_execution=5\n",
    "  )\n",
    "  metrics = model.evaluate(ds)\n",
    "  print('Metrics Before Fit:', metrics)\n",
    "\n",
    "  model.fit(ds, epochs=10, verbose=2)\n",
    "\n",
    "  # we can go ahead and predict some values\n",
    "  y_pred = model.predict(ds_test)\n",
    "  print('Predictions Shape:', y_pred.shape)\n",
    "\n",
    "  metrics = model.evaluate(ds)\n",
    "  print('Metrics After:', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1xjwjCo9za9",
    "outputId": "deb77ce3-0657-45a9-e8c4-113ce6679de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "101/101 - 0s - 482ms/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 276ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 255ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 224ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 234ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 221ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 249ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 244ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 240ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 245ms/epoch - 2ms/step\n",
      "Epoch 1/10\n",
      "101/101 - 0s - 219ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 225ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 243ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 222ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 225ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 280ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 272ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 272ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 262ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 244ms/epoch - 2ms/step\n",
      "Epoch 1/10\n",
      "101/101 - 0s - 229ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 278ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 260ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 278ms/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 270ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 249ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 244ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 272ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 269ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 225ms/epoch - 2ms/step\n",
      "Epoch 1/10\n",
      "101/101 - 0s - 240ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 275ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 233ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 225ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 239ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 246ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 227ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 262ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 245ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 230ms/epoch - 2ms/step\n",
      "Epoch 1/10\n",
      "101/101 - 0s - 246ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 237ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 227ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 224ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 250ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 247ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 224ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 240ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 274ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 261ms/epoch - 3ms/step\n",
      "Epoch 1/10\n",
      "101/101 - 0s - 268ms/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "101/101 - 0s - 268ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "101/101 - 0s - 266ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "101/101 - 0s - 266ms/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "101/101 - 0s - 281ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "101/101 - 0s - 270ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "101/101 - 0s - 263ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "101/101 - 0s - 251ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "101/101 - 0s - 260ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "101/101 - 0s - 271ms/epoch - 3ms/step\n",
      "1 loop, best of 5: 3.02 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model.fit(ds, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhCq4p88bkga"
   },
   "source": [
    "As you can see, `model.fit()` now runs *significantly* faster than it did before implementing our performance enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg_zGgxD-BO1"
   },
   "source": [
    "# Conclusion\n",
    "The `keras.Model` class contains a encapsulates set of functionality related to training.  Due to this, the lifecycle of the `keras.Model` class is significantly complex.\n",
    "\n",
    "The `SimplifiedModel` class we implemented shows how the core functionality of the `keras.Model` works, while still remaining terse and readable.  while the `SimplifiedModel` class is missing a large portion of the true `keras.Model` class's functionality, it is still as useful starting point for implementing custom training loops that work with TensorFlow distribution strategies.\n",
    "\n",
    "A forkable template using the `SimplifiedModel` class is available at [https://github.com/lukewood/ModelWalkthrough](https://github.com/lukewood/ModelWalkthrough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxj-jgCd-04A"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ModelWalkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
